{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import scipy.io\r\n",
    "import pandas as pd\r\n",
    "import json\r\n",
    "from typing import List, Callable\r\n",
    "import run_classifiers\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "corrs_location = '../corrs'\r\n",
    "A_mats_location = '../A_mats'\r\n",
    "\r\n",
    "participants = pd.read_table('../SRPBS_OPEN/participants.tsv')\r\n",
    "participants = participants.dropna()\r\n",
    "\r\n",
    "train_sites = ['COI','KUT','SWA','UTO']\r\n",
    "test_sites = ['ATT','ATV','CIN','HKH','HRC','HUH','KTT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def participant_id_to_number(participant_id: str) -> int:\r\n",
    "    return int(participant_id.split('-')[1])\r\n",
    "\r\n",
    "def extract_participants_id(participants: pd.DataFrame, id: int) -> pd.DataFrame:\r\n",
    "\treturn participants[[participant_id_to_number(name) == id for name in participants.participant_id.to_numpy()]]\r\n",
    "\r\n",
    "def unpair(pairs: List[List[int]]) -> List[int]:\r\n",
    "\treturn [id for pair in pairs for id in pair]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2 types of data (A matrices or correlations)\r\n",
    "* 2 outlier choices (remove or not)\r\n",
    "* 3 dataset subsets (all data, all pairs, perfect pairs)\r\n",
    "\r\n",
    "In total - we have 2x2x3 = 12 datasets to feed into the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset Analysis/dataset_all.txt','r') as f:\r\n",
    "\tids_all_all = np.array(json.loads(f.read()))\r\n",
    "with open('../Dataset Analysis/dataset_inliers.txt','r') as f:\r\n",
    "\tids_inliers_all = np.array(json.loads(f.read()))\r\n",
    "with open('../Dataset Analysis/all_pairs.txt','r') as f:\r\n",
    "\tids_all_pairs = np.array(json.loads(f.read()))\r\n",
    "with open('../Dataset Analysis/all_inliers_pairs.txt','r') as f:\r\n",
    "\tids_inliers_pairs = np.array(json.loads(f.read()))\r\n",
    "with open('../Dataset Analysis/all_perfect_pairs.txt','r') as f:\r\n",
    "\tids_all_perfectpairs = np.array(json.loads(f.read()))\r\n",
    "with open('../Dataset Analysis/all_inliers_perfect_pairs.txt','r') as f:\r\n",
    "\tids_inliers_perfectpairs = np.array(json.loads(f.read()))\r\n",
    "\r\n",
    "dataset_types = {\r\n",
    "\t'all': ids_all_all, \r\n",
    "\t'inliers': ids_inliers_all, \r\n",
    "\t# 'all_pairs': ids_all_pairs,\r\n",
    "\t# 'inliers_pairs': ids_inliers_pairs, \r\n",
    "\t# 'all_perfectpairs': ids_all_perfectpairs, \r\n",
    "\t# 'inliers_perfectpairs': ids_inliers_perfectpairs\r\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_correlation(subject: int) -> np.ndarray:\r\n",
    "\treturn np.nan_to_num(scipy.io.loadmat(f'{corrs_location}/correlation_components_{subject:04d}.mat')['corr_components'].reshape(1,-1))\r\n",
    "\r\n",
    "def read_A_matrix(subject: int) -> np.ndarray:\r\n",
    "\treturn scipy.io.loadmat(f'{A_mats_location}/dcm_A_{subject:04d}.mat')['A'].reshape(1,-1)\r\n",
    "\r\n",
    "load_data_functions = {'corr': read_correlation, 'Amat': read_A_matrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(ids: np.ndarray, load_data_function: Callable[[int], np.ndarray]):\r\n",
    "\tif ids.ndim == 1: # single list of participants\r\n",
    "\t\tparticpant_data = [extract_participants_id(participants, id) for id in ids]\r\n",
    "\telse: # pairs of participants\r\n",
    "\t\tparticpant_data = [extract_participants_id(participants, id) for pair in ids for id in pair]\r\n",
    "\t\tids = ids.flatten()\r\n",
    "\tparticpants_train = [participant.site.to_numpy() in train_sites for participant in particpant_data]\r\n",
    "\tparticpants_test = [participant.site.to_numpy() in test_sites for participant in particpant_data]\r\n",
    "\r\n",
    "\tX_train = np.vstack([load_data_function(id) for id in ids[particpants_train]])\r\n",
    "\tX_test = np.vstack([load_data_function(id) for id in ids[particpants_test]])\r\n",
    "\tlabels = np.array(list(map(lambda participant: int(participant.diag // 2), particpant_data)))\r\n",
    "\tY_train = labels[particpants_train]\r\n",
    "\tY_test = labels[particpants_test]\r\n",
    "\treturn X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\r\n",
    "\r\n",
    "for dataset_name, ids in dataset_types.items():\r\n",
    "\tfor function_name, load_data_function in load_data_functions.items():\r\n",
    "\t\tdatasets.append([f'{dataset_name}_{function_name}', *load_dataset(ids, load_data_function)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_corr: (total - 1043)\n",
      "\ttrain - [X: (626, 70876), Y: (626,)] (60.0%)\n",
      "\ttest -  [X: (417, 70876), Y: (417,)] (40.0%)\n",
      "all_Amat: (total - 1043)\n",
      "\ttrain - [X: (626, 142129), Y: (626,)] (60.0%)\n",
      "\ttest -  [X: (417, 142129), Y: (417,)] (40.0%)\n",
      "inliers_corr: (total - 994)\n",
      "\ttrain - [X: (589, 70876), Y: (589,)] (59.3%)\n",
      "\ttest -  [X: (405, 70876), Y: (405,)] (40.7%)\n",
      "inliers_Amat: (total - 994)\n",
      "\ttrain - [X: (589, 142129), Y: (589,)] (59.3%)\n",
      "\ttest -  [X: (405, 142129), Y: (405,)] (40.7%)\n",
      "all has 1043 participants\n",
      "inliers has 994 participants\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\r\n",
    "\ttrain_proportion = 100*dataset[3].shape[0]/(dataset[3].shape[0] + dataset[4].shape[0])\r\n",
    "\ttest_proportion = 100*dataset[4].shape[0]/(dataset[3].shape[0] + dataset[4].shape[0])\r\n",
    "\tprint(f'{dataset[0]}: (total - {dataset[3].shape[0] + dataset[4].shape[0]})\\n\\ttrain - [X: {dataset[1].shape}, Y: {dataset[3].shape}] ({train_proportion:.1f}%)\\n\\ttest -  [X: {dataset[2].shape}, Y: {dataset[4].shape}] ({test_proportion:.1f}%)')\r\n",
    "for dataset_name, ids in dataset_types.items():\r\n",
    "\tprint(f'{dataset_name} has {ids.size} participants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset all_corr\n",
      "... with Lasso_cv classifier\n",
      "Running outer loop 0\n",
      "Running subsample 0\n",
      "alpha validation scores = [0.7139601139601138, 0.7142450142450143, 0.7061253561253562, 0.6676638176638175], best alpha = 0.01\n",
      "Running subsample 1\n",
      "alpha validation scores = [0.7250712250712251, 0.7401709401709401, 0.7403133903133903, 0.7478632478632479], best alpha = 1.0\n",
      "Running subsample 2\n",
      "alpha validation scores = [0.717094017094017, 0.7245014245014245, 0.7283475783475784, 0.7015669515669516], best alpha = 0.1\n",
      "Running subsample 3\n",
      "alpha validation scores = [0.7556980056980057, 0.7670940170940171, 0.7403133903133903, 0.7098290598290599], best alpha = 0.01\n",
      "Running subsample 4\n",
      "alpha validation scores = [0.7249287749287749, 0.7170940170940171, 0.7249287749287749, 0.7098290598290597], best alpha = 0.001\n",
      "Running subsample 5\n",
      "alpha validation scores = [0.7703703703703704, 0.7437321937321938, 0.7437321937321938, 0.754985754985755], best alpha = 0.001\n",
      "Running subsample 6\n",
      "alpha validation scores = [0.7249287749287749, 0.7132478632478632, 0.7212250712250713, 0.7133903133903134], best alpha = 0.001\n",
      "Running subsample 7\n",
      "alpha validation scores = [0.7477207977207977, 0.7515669515669516, 0.7746438746438746, 0.7595441595441595], best alpha = 0.1\n",
      "Running subsample 8\n",
      "alpha validation scores = [0.7709401709401709, 0.7707977207977208, 0.7666666666666666, 0.763105413105413], best alpha = 0.001\n",
      "Running subsample 9\n",
      "alpha validation scores = [0.7138176638176639, 0.7175213675213675, 0.7326210826210826, 0.767094017094017], best alpha = 1.0\n",
      "Running outer loop 1\n",
      "Running subsample 10\n",
      "alpha validation scores = [0.7672364672364673, 0.7556980056980057, 0.7210826210826211, 0.7207977207977209], best alpha = 0.001\n",
      "Running subsample 11\n",
      "alpha validation scores = [0.7441595441595441, 0.7441595441595441, 0.743874643874644, 0.7364672364672366], best alpha = 0.001\n",
      "Running subsample 12\n",
      "alpha validation scores = [0.7749287749287749, 0.7786324786324788, 0.7595441595441595, 0.7515669515669515], best alpha = 0.01\n",
      "Running subsample 13\n",
      "alpha validation scores = [0.6796296296296296, 0.656837606837607, 0.6571225071225072, 0.6378917378917379], best alpha = 0.001\n",
      "Running subsample 14\n",
      "alpha validation scores = [0.770940170940171, 0.7361823361823362, 0.7287749287749288, 0.7287749287749288], best alpha = 0.001\n",
      "Running subsample 15\n",
      "alpha validation scores = [0.7548433048433049, 0.7475783475783475, 0.7135327635327635, 0.7246438746438747], best alpha = 0.001\n",
      "Running subsample 16\n",
      "alpha validation scores = [0.7367521367521368, 0.7289173789173788, 0.7284900284900285, 0.7247863247863249], best alpha = 0.001\n",
      "Running subsample 17\n",
      "alpha validation scores = [0.732905982905983, 0.7062678062678063, 0.6868945868945869, 0.7021367521367521], best alpha = 0.001\n",
      "Running subsample 18\n",
      "alpha validation scores = [0.7173789173789175, 0.7206552706552707, 0.6903133903133903, 0.6900284900284899], best alpha = 0.01\n",
      "Running subsample 19\n",
      "alpha validation scores = [0.7106837606837606, 0.7142450142450143, 0.7068376068376068, 0.7103988603988605], best alpha = 0.01\n",
      "Running outer loop 2\n",
      "Running subsample 20\n",
      "alpha validation scores = [0.7096866096866097, 0.7212250712250712, 0.7216524216524217, 0.7024216524216526], best alpha = 0.1\n",
      "Running subsample 21\n",
      "alpha validation scores = [0.7448717948717949, 0.7715099715099715, 0.7790598290598291, 0.7638176638176638], best alpha = 0.1\n",
      "Running subsample 22\n",
      "alpha validation scores = [0.7823361823361824, 0.7747863247863248, 0.7864672364672366, 0.7829059829059829], best alpha = 0.1\n",
      "Running subsample 23\n",
      "alpha validation scores = [0.7566951566951567, 0.7564102564102564, 0.7524216524216524, 0.7254985754985754], best alpha = 0.001\n",
      "Running subsample 24\n",
      "alpha validation scores = [0.7178062678062679, 0.7145299145299145, 0.7142450142450143, 0.7217948717948719], best alpha = 1.0\n",
      "Running subsample 25\n",
      "alpha validation scores = [0.733048433048433, 0.7363247863247864, 0.7292022792022792, 0.7443019943019944], best alpha = 1.0\n",
      "Running subsample 26\n",
      "alpha validation scores = [0.748005698005698, 0.748005698005698, 0.7673789173789174, 0.7253561253561254], best alpha = 0.1\n",
      "Running subsample 27\n",
      "alpha validation scores = [0.7555555555555555, 0.7480056980056979, 0.7592592592592593, 0.7403133903133903], best alpha = 0.1\n",
      "Running subsample 28\n",
      "alpha validation scores = [0.7173789173789175, 0.70997150997151, 0.6715099715099717, 0.6297720797720798], best alpha = 0.001\n",
      "Running subsample 29\n",
      "alpha validation scores = [0.7176638176638176, 0.7443019943019943, 0.6871794871794872, 0.6870370370370369], best alpha = 0.01\n",
      "Running outer loop 3\n",
      "Running subsample 30\n",
      "alpha validation scores = [0.7556980056980057, 0.7515669515669516, 0.7403133903133903, 0.7475783475783476], best alpha = 0.001\n",
      "Running subsample 31\n",
      "alpha validation scores = [0.7484330484330484, 0.7521367521367521, 0.7715099715099715, 0.7481481481481481], best alpha = 0.1\n",
      "Running subsample 32\n",
      "alpha validation scores = [0.7403133903133903, 0.7705128205128204, 0.774074074074074, 0.7894586894586894], best alpha = 1.0\n",
      "Running subsample 33\n",
      "alpha validation scores = [0.7324786324786324, 0.705840455840456, 0.7286324786324785, 0.7091168091168092], best alpha = 0.001\n",
      "Running subsample 34\n",
      "alpha validation scores = [0.7668091168091168, 0.7743589743589744, 0.7789173789173789, 0.7441595441595441], best alpha = 0.1\n",
      "Running subsample 35\n",
      "alpha validation scores = [0.7256410256410257, 0.7561253561253561, 0.7331908831908832, 0.736894586894587], best alpha = 0.01\n",
      "Running subsample 36\n",
      "alpha validation scores = [0.7675213675213675, 0.7670940170940171, 0.7521367521367521, 0.751994301994302], best alpha = 0.001\n",
      "Running subsample 37\n",
      "alpha validation scores = [0.733048433048433, 0.7135327635327636, 0.6944444444444444, 0.6789173789173789], best alpha = 0.001\n",
      "Running subsample 38\n",
      "alpha validation scores = [0.7787749287749287, 0.766951566951567, 0.7632478632478632, 0.7783475783475783], best alpha = 0.001\n",
      "Running subsample 39\n",
      "alpha validation scores = [0.7787749287749287, 0.7632478632478632, 0.778917378917379, 0.7478632478632479], best alpha = 0.1\n",
      "Running outer loop 4\n",
      "Running subsample 40\n",
      "alpha validation scores = [0.7212250712250713, 0.7173789173789175, 0.7103988603988605, 0.6874643874643873], best alpha = 0.001\n",
      "Running subsample 41\n",
      "alpha validation scores = [0.7787749287749288, 0.771082621082621, 0.782905982905983, 0.7676638176638176], best alpha = 0.1\n",
      "Running subsample 42\n",
      "alpha validation scores = [0.7096866096866097, 0.7249287749287749, 0.7175213675213674, 0.705982905982906], best alpha = 0.01\n",
      "Running subsample 43\n",
      "alpha validation scores = [0.7373219373219373, 0.7138176638176639, 0.7135327635327635, 0.70997150997151], best alpha = 0.001\n",
      "Running subsample 44\n",
      "alpha validation scores = [0.7485754985754985, 0.7445868945868945, 0.7670940170940171, 0.7787749287749287], best alpha = 1.0\n",
      "Running subsample 45\n",
      "alpha validation scores = [0.7293447293447295, 0.7294871794871796, 0.7025641025641025, 0.6948717948717948], best alpha = 0.01\n",
      "Running subsample 46\n",
      "alpha validation scores = [0.755982905982906, 0.7517094017094017, 0.733048433048433, 0.7441595441595442], best alpha = 0.001\n",
      "Running subsample 47\n",
      "alpha validation scores = [0.748005698005698, 0.7367521367521367, 0.7404558404558405, 0.7324786324786324], best alpha = 0.001\n",
      "Running subsample 48\n",
      "alpha validation scores = [0.783048433048433, 0.7752136752136751, 0.7792022792022791, 0.7867521367521368], best alpha = 1.0\n",
      "Running subsample 49\n",
      "alpha validation scores = [0.8054131054131053, 0.8052706552706553, 0.8017094017094017, 0.7364672364672364], best alpha = 0.001\n",
      "Running outer loop 5\n",
      "Running subsample 50\n",
      "alpha validation scores = [0.7517094017094016, 0.7669515669515669, 0.7821937321937321, 0.7397435897435898], best alpha = 0.1\n",
      "Running subsample 51\n",
      "alpha validation scores = [0.7668091168091168, 0.7819088319088319, 0.7893162393162394, 0.7817663817663817], best alpha = 0.1\n",
      "Running subsample 52\n",
      "alpha validation scores = [0.7327635327635328, 0.7515669515669516, 0.7518518518518519, 0.748005698005698], best alpha = 0.1\n",
      "Running subsample 53\n",
      "alpha validation scores = [0.7511396011396012, 0.7321937321937322, 0.7435897435897435, 0.7321937321937323], best alpha = 0.001\n",
      "Running subsample 54\n",
      "alpha validation scores = [0.7857549857549857, 0.8051282051282052, 0.8009971509971511, 0.8048433048433049], best alpha = 0.01\n",
      "Running subsample 55\n",
      "alpha validation scores = [0.7253561253561254, 0.7408831908831909, 0.728917378917379, 0.7361823361823362], best alpha = 0.01\n",
      "Running subsample 56\n",
      "alpha validation scores = [0.6911680911680912, 0.6796296296296297, 0.6793447293447293, 0.6907407407407409], best alpha = 0.001\n",
      "Running subsample 57\n",
      "alpha validation scores = [0.743874643874644, 0.766951566951567, 0.7706552706552706, 0.7551282051282051], best alpha = 0.1\n",
      "Running subsample 58\n",
      "alpha validation scores = [0.7443019943019943, 0.7633903133903134, 0.7286324786324786, 0.7250712250712251], best alpha = 0.01\n",
      "Running subsample 59\n",
      "alpha validation scores = [0.733048433048433, 0.7142450142450143, 0.7256410256410256, 0.732905982905983], best alpha = 0.001\n",
      "Running outer loop 6\n",
      "Running subsample 60\n",
      "alpha validation scores = [0.708119658119658, 0.7270655270655271, 0.7309116809116809, 0.6890313390313391], best alpha = 0.1\n",
      "Running subsample 61\n",
      "alpha validation scores = [0.724074074074074, 0.7350427350427351, 0.7274928774928775, 0.707977207977208], best alpha = 0.01\n",
      "Running subsample 62\n",
      "alpha validation scores = [0.7266381766381766, 0.7383190883190883, 0.7417378917378917, 0.7381766381766383], best alpha = 0.1\n",
      "Running subsample 63\n",
      "alpha validation scores = [0.7007122507122506, 0.7193732193732194, 0.7039886039886041, 0.7267806267806268], best alpha = 1.0\n",
      "Running subsample 64\n",
      "alpha validation scores = [0.7414529914529915, 0.7488603988603988, 0.7494301994301995, 0.7534188034188034], best alpha = 1.0\n",
      "Running subsample 65\n",
      "alpha validation scores = [0.7267806267806268, 0.7532763532763533, 0.7377492877492877, 0.7034188034188034], best alpha = 0.01\n",
      "Running subsample 66\n",
      "alpha validation scores = [0.78005698005698, 0.8142450142450143, 0.8253561253561253, 0.8098290598290598], best alpha = 0.1\n",
      "Running subsample 67\n",
      "alpha validation scores = [0.7387464387464387, 0.7658119658119658, 0.7316239316239316, 0.7462962962962962], best alpha = 0.01\n",
      "Running subsample 68\n",
      "alpha validation scores = [0.7085470085470085, 0.7501424501424501, 0.7656695156695157, 0.7507122507122507], best alpha = 0.1\n",
      "Running subsample 69\n",
      "alpha validation scores = [0.7156695156695158, 0.7272079772079773, 0.7047008547008546, 0.7082621082621083], best alpha = 0.01\n",
      "Running outer loop 7\n",
      "Running subsample 70\n",
      "alpha validation scores = [0.7, 0.7037037037037038, 0.7085470085470086, 0.6891737891737891], best alpha = 0.1\n",
      "Running subsample 71\n",
      "alpha validation scores = [0.7954415954415954, 0.8148148148148147, 0.7772079772079772, 0.7693732193732193], best alpha = 0.01\n",
      "Running subsample 72\n",
      "alpha validation scores = [0.7622507122507122, 0.7962962962962963, 0.7846153846153846, 0.7656695156695156], best alpha = 0.01\n",
      "Running subsample 73\n",
      "alpha validation scores = [0.7344729344729346, 0.7353276353276353, 0.7122507122507123, 0.7349002849002849], best alpha = 0.01\n",
      "Running subsample 74\n",
      "alpha validation scores = [0.7501424501424502, 0.7084045584045585, 0.7160968660968661, 0.6816239316239315], best alpha = 0.001\n",
      "Running subsample 75\n",
      "alpha validation scores = [0.7846153846153847, 0.7769230769230769, 0.7806267806267806, 0.7467236467236468], best alpha = 0.001\n",
      "Running subsample 76\n",
      "alpha validation scores = [0.7801994301994302, 0.8028490028490027, 0.8145299145299145, 0.7537037037037038], best alpha = 0.1\n",
      "Running subsample 77\n",
      "alpha validation scores = [0.7007122507122507, 0.7232193732193732, 0.7307692307692307, 0.7230769230769231], best alpha = 0.1\n",
      "Running subsample 78\n",
      "alpha validation scores = [0.7266381766381766, 0.7230769230769232, 0.7534188034188034, 0.7115384615384616], best alpha = 0.1\n",
      "Running subsample 79\n",
      "alpha validation scores = [0.7233618233618234, 0.7538461538461538, 0.7574074074074074, 0.7424501424501424], best alpha = 0.1\n",
      "Running outer loop 8\n",
      "Running subsample 80\n",
      "alpha validation scores = [0.7652421652421652, 0.7461538461538462, 0.7316239316239316, 0.7015669515669516], best alpha = 0.001\n",
      "Running subsample 81\n",
      "alpha validation scores = [0.7645299145299146, 0.7717948717948718, 0.7911680911680912, 0.7797720797720797], best alpha = 0.1\n",
      "Running subsample 82\n",
      "alpha validation scores = [0.734188034188034, 0.7381766381766381, 0.7418803418803418, 0.722934472934473], best alpha = 0.1\n",
      "Running subsample 83\n",
      "alpha validation scores = [0.7645299145299145, 0.7576923076923078, 0.7803418803418803, 0.7535612535612535], best alpha = 0.1\n",
      "Running subsample 84\n",
      "alpha validation scores = [0.7454415954415954, 0.7537037037037038, 0.7688034188034187, 0.7495726495726496], best alpha = 0.1\n",
      "Running subsample 85\n",
      "alpha validation scores = [0.7464387464387464, 0.760968660968661, 0.7574074074074074, 0.7267806267806268], best alpha = 0.01\n",
      "Running subsample 86\n",
      "alpha validation scores = [0.7578347578347578, 0.7655270655270655, 0.7613960113960113, 0.7572649572649571], best alpha = 0.01\n",
      "Running subsample 87\n",
      "alpha validation scores = [0.7128205128205127, 0.6938746438746438, 0.7089743589743589, 0.6858974358974359], best alpha = 0.001\n",
      "Running subsample 88\n",
      "alpha validation scores = [0.756980056980057, 0.7723646723646723, 0.7502849002849002, 0.7344729344729345], best alpha = 0.01\n",
      "Running subsample 89\n",
      "alpha validation scores = [0.7692307692307692, 0.7807692307692307, 0.7693732193732193, 0.7388888888888888], best alpha = 0.01\n",
      "Running outer loop 9\n",
      "Running subsample 90\n",
      "alpha validation scores = [0.72991452991453, 0.7568376068376068, 0.7568376068376068, 0.7722222222222224], best alpha = 1.0\n",
      "Running subsample 91\n",
      "alpha validation scores = [0.7618233618233619, 0.7801994301994302, 0.7990028490028491, 0.7951566951566951], best alpha = 0.1\n",
      "Running subsample 92\n",
      "alpha validation scores = [0.7502849002849002, 0.7725071225071225, 0.7685185185185186, 0.7650997150997152], best alpha = 0.01\n",
      "Running subsample 93\n",
      "alpha validation scores = [0.7492877492877493, 0.7571225071225072, 0.7343304843304844, 0.7035612535612537], best alpha = 0.01\n",
      "Running subsample 94\n",
      "alpha validation scores = [0.8145299145299145, 0.7717948717948718, 0.7907407407407407, 0.7638176638176637], best alpha = 0.001\n",
      "Running subsample 95\n",
      "alpha validation scores = [0.7528490028490029, 0.7717948717948718, 0.7836182336182336, 0.795014245014245], best alpha = 1.0\n",
      "Running subsample 96\n",
      "alpha validation scores = [0.7230769230769231, 0.7380341880341881, 0.7227920227920228, 0.710968660968661], best alpha = 0.01\n",
      "Running subsample 97\n",
      "alpha validation scores = [0.7571225071225072, 0.7462962962962962, 0.7465811965811965, 0.75], best alpha = 0.001\n",
      "Running subsample 98\n",
      "alpha validation scores = [0.7346153846153846, 0.7264957264957264, 0.7343304843304843, 0.738034188034188], best alpha = 1.0\n",
      "Running subsample 99\n",
      "alpha validation scores = [0.7230769230769231, 0.7613960113960114, 0.7612535612535613, 0.7642450142450142], best alpha = 1.0\n",
      "prediction train = [0.   0.   0.01 0.   0.   0.   0.   0.   0.29 0.04 0.04 0.   0.11 0.01\n",
      " 0.   0.07 0.   0.06 0.02 0.43 0.04 0.   0.02 0.   0.   0.   0.15 0.03\n",
      " 0.02 0.   0.08 0.   0.01 0.07 0.   0.07 0.13 0.02 0.11 0.   0.21 0.\n",
      " 0.   0.   0.   0.   0.09 0.07 0.08 0.   0.08 0.01 0.03 0.02 0.03 0.03\n",
      " 0.01 0.   0.17 0.   0.33 0.   0.01 0.04 0.   0.   0.   0.4  0.   0.02\n",
      " 0.02 0.   0.07 0.   0.02 0.   0.   0.   0.01 0.08 0.05 0.   0.01 0.\n",
      " 0.19 0.   0.08 0.   0.31 0.   0.   0.   0.   0.   0.12 0.   0.01 0.\n",
      " 0.2  0.08 0.   0.47 0.97 0.73 0.99 0.73 0.95 0.71 0.94 0.63 0.91 0.31\n",
      " 1.   0.55 1.   0.02 0.97 0.54 0.53 0.98 0.24 1.   0.06 0.96 0.62 0.98\n",
      " 0.47 1.   0.41 1.   0.63 1.   0.78 0.92 0.8  1.   0.73 0.96 0.29 0.95\n",
      " 0.74 0.97 0.69 0.98 0.71 1.   0.48 0.95 0.78 1.   0.66 1.   0.76 0.97\n",
      " 0.32 1.   0.56 0.99 0.58 0.99 0.7  0.94 0.61 1.   0.64 0.96 0.39 1.\n",
      " 0.52 0.99 0.19 0.99 0.64 1.   0.55 0.98 0.66 0.97 0.28 0.98 0.68 1.\n",
      " 0.1  1.   0.64 0.92 0.73 0.99 0.36 1.   0.23 0.98 0.6  1.   0.13 1.\n",
      " 0.66 0.94 0.32 0.99 0.47 0.99 0.59 1.   0.04 1.   0.37 0.93 0.27 0.99\n",
      " 0.75 0.09 1.   0.53 1.   0.76 1.   0.25 0.96 0.57 0.99 0.23 0.47 0.11\n",
      " 0.28 0.57 0.49 0.29 0.6  0.52 0.56 0.45 0.59 0.66 0.29 0.53 0.22 0.2\n",
      " 0.77 0.33 0.15 0.3  0.54 0.49 0.55 0.58 0.29 0.63 0.74 0.22 0.67 0.69\n",
      " 0.54 0.55 0.62 0.26 0.08 0.33 0.38 0.4  0.58 0.47 0.73 0.78 0.16 0.64\n",
      " 0.73 0.53 0.73 0.09 0.11 0.59 0.53 0.33 0.58 0.08 0.59 0.43 0.52 0.19\n",
      " 0.2  0.45 0.35 0.22 0.   0.43 0.21 0.08 0.12 0.   0.01 0.3  0.76 0.08\n",
      " 0.06 0.   0.09 0.39 0.38 0.05 0.26 0.01 0.   0.   0.04 0.01 0.   0.2\n",
      " 0.   0.33 0.04 0.01 0.   0.   0.02 0.58 0.3  0.   0.17 0.48 0.08 0.09\n",
      " 0.02 0.08 0.   0.13 0.02 0.04 0.05 0.25 0.   0.   0.   0.17 0.09 0.01\n",
      " 0.04 0.01 0.05 0.04 0.67 0.   0.09 0.07 0.   0.   0.01 0.11 0.05 0.48\n",
      " 0.   0.   0.   0.32 0.07 0.21 0.31 0.13 0.01 0.02 0.15 0.04 0.   0.\n",
      " 0.14 0.2  0.   0.03 0.01 0.01 0.25 0.03 0.19 0.04 0.   0.   0.02 0.5\n",
      " 0.18 0.04 0.   0.06 0.37 0.   0.11 0.   0.17 0.08 0.   0.06 0.01 0.01\n",
      " 0.57 0.   0.01 0.1  0.1  0.   0.16 0.   0.   0.25 0.12 0.11 0.19 0.14\n",
      " 0.   0.12 0.   0.06 0.   0.07 0.55 0.14 0.12 0.14 0.4  0.11 0.4  0.\n",
      " 0.17 0.02 0.   0.03 0.06 0.09 0.1  0.   0.2  0.   0.01 0.03 0.03 0.66\n",
      " 0.05 0.   0.07 0.25 0.   0.1  0.07 0.01 0.35 1.   0.95 1.   0.92 0.99\n",
      " 0.9  0.91 0.98 0.91 0.9  0.95 0.9  0.9  0.9  0.94 0.93 1.   1.   0.99\n",
      " 0.99 1.   0.99 1.   1.   1.   0.93 0.9  0.61 0.64 0.47 0.78 0.73 0.76\n",
      " 0.76 0.54 0.36 0.15 0.43 0.61 0.31 0.72 0.51 0.29 0.18 0.14 0.18 0.38\n",
      " 0.57 0.68 0.68 0.73 0.6  0.67 0.42 0.64 0.4  0.62 0.42 0.44 0.32 0.73\n",
      " 0.19 0.41 0.36 0.32 0.39 0.62 0.77 0.65 0.34 0.48 0.62 0.29 0.72 0.59\n",
      " 0.35 0.56 0.12 0.09 0.79 0.67 0.76 0.49 0.76 0.75 0.66 0.77 0.32 0.72\n",
      " 0.63 0.62 0.51 0.67 0.44 0.72 0.04 0.79 0.05 0.81 0.01 0.05 0.37 0.57\n",
      " 0.6  0.65 0.58 0.7  0.32 0.66 0.1  0.19 0.71 0.43 0.46 0.56 0.12 0.62\n",
      " 0.73 0.26 0.2  0.36 0.78 0.74 0.99 0.97 0.98 0.98 1.   1.   0.91 1.\n",
      " 1.   0.99 1.   0.95 0.95 1.   1.   0.99 1.   1.   1.   1.   1.   0.98\n",
      " 1.   0.99 1.   1.   1.   1.   0.99 0.99 0.97 1.   0.99 1.   1.   1.\n",
      " 1.   0.96 1.   0.94 1.   1.   1.   1.   0.94 1.   1.   0.95 1.   1.\n",
      " 1.   0.99 1.   0.94 0.99 1.   0.97 0.99 1.   0.98]\n",
      "prediction test = [0.44 0.98 0.17 0.23 0.68 0.34 0.45 0.49 0.33 0.51 0.25 0.41 0.22 0.58\n",
      " 0.51 0.49 0.86 0.84 0.24 0.29 0.04 0.89 0.55 0.39 0.21 0.55 0.11 0.78\n",
      " 0.82 0.33 0.53 0.17 0.55 0.81 0.16 0.33 0.92 0.82 0.01 0.66 0.06 0.11\n",
      " 0.76 0.75 0.26 0.75 0.1  0.6  0.34 0.73 0.02 0.73 0.09 0.07 0.39 0.15\n",
      " 0.14 0.93 0.06 0.37 0.02 0.55 0.1  0.76 0.1  0.43 0.39 0.45 0.34 0.25\n",
      " 0.36 0.28 0.13 0.29 0.76 0.85 0.84 0.75 0.84 0.65 0.85 0.42 0.31 0.05\n",
      " 0.38 0.4  0.5  0.07 0.38 0.21 0.36 0.72 0.53 0.66 0.23 0.81 0.91 0.13\n",
      " 0.5  0.97 0.84 0.47 0.28 0.77 0.39 0.67 0.02 0.39 0.81 0.42 0.08 0.85\n",
      " 0.48 0.61 0.28 0.22 0.84 0.25 0.81 0.46 0.87 0.12 0.77 0.16 0.18 0.68\n",
      " 0.7  0.57 0.75 0.73 0.37 1.   0.92 0.86 0.12 0.82 0.66 0.76 0.25 0.48\n",
      " 0.28 0.31 0.35 0.26 0.15 0.31 0.1  0.93 0.67 0.81 0.32 0.88 0.96 0.78\n",
      " 0.34 0.53 0.08 0.75 0.71 0.15 0.98 0.15 0.36 0.74 0.79 0.56 0.35 0.82\n",
      " 0.   0.09 0.82 0.98 0.63 0.36 0.51 0.9  0.85 0.6  0.54 0.22 0.81 0.27\n",
      " 0.88 0.93 0.57 0.1  0.92 0.6  0.29 0.38 0.82 0.85 0.86 0.24 0.3  0.02\n",
      " 0.92 0.21 0.32 0.38 0.24 0.53 0.81 0.89 0.54 0.85 0.77 0.54 0.45 0.57\n",
      " 0.64 0.59 0.82 0.7  0.46 0.49 0.72 0.32 0.12 0.34 0.32 0.13 0.75 0.7\n",
      " 0.23 0.1  0.23 0.76 0.6  0.4  0.12 0.32 0.85 0.19 0.36 0.2  0.8  0.08\n",
      " 0.59 0.56 0.15 0.51 0.96 0.25 0.34 0.36 0.69 0.48 0.16 0.93 0.72 0.97\n",
      " 0.34 0.61 0.98 0.78 0.88 0.23 0.78 0.08 0.17 0.19 0.8  0.9  0.57 0.89\n",
      " 0.95 0.69 0.77 0.71 0.47 0.56 0.5  0.82 0.72 0.83 0.53 0.75 0.02 0.89\n",
      " 0.86 0.26 0.43 0.24 0.99 0.99 0.4  0.84 0.91 0.4  0.57 0.56 0.96 0.62\n",
      " 0.56 0.73 0.91 0.25 0.51 0.94 0.82 0.67 0.26 0.07 0.9  0.6  0.97 0.59\n",
      " 0.47 0.15 0.56 0.69 0.77 0.01 0.06 0.07 0.45 0.06 0.94 0.64 0.99 0.99\n",
      " 1.   0.97 0.86 0.91 0.73 0.23 0.04 0.   0.42 0.17 0.01 0.02 0.   0.44\n",
      " 0.06 0.02 0.   0.63 0.19 0.02 0.22 0.44 0.19 0.   0.   0.1  0.01 0.88\n",
      " 0.09 0.01 0.69 0.   0.03 0.12 0.44 0.29 0.11 0.03 0.   0.02 0.03 0.05\n",
      " 0.41 0.27 0.28 0.73 0.   0.5  0.36 0.03 0.   0.   0.41 0.04 0.   0.\n",
      " 0.23 0.09 0.09 0.37 0.61 0.03 0.08 0.22 0.13 0.07 0.13 0.79 0.06 0.66\n",
      " 0.89 0.02 0.39 0.09 0.26 0.1  0.92 0.85 0.02 0.84 0.37 0.16 0.94 0.6\n",
      " 0.83 0.14 0.5  0.78 0.   0.58 0.01 0.88 0.85 0.56 0.04]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'update'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-1c735986d523>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_classifiers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_classifiers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'100_classifier_scores'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Uni\\UZH\\Translational Neuromodelling\\Project\\Code\\Classifier\\run_classifiers.py\u001b[0m in \u001b[0;36mrun_classifiers\u001b[1;34m(datasets)\u001b[0m\n\u001b[0;32m    140\u001b[0m \t\t\t}) \n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m                 \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mds_name\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mdataset_scores\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'update'"
     ]
    }
   ],
   "source": [
    "scores = run_classifiers.run_classifiers([datasets[0]])\r\n",
    "np.save('100_classifier_scores', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = '[0.   0.   0.01 0.   0.   0.   0.   0.   0.29 0.04 0.04 0.   0.11 0.01 \\\r\n",
    " 0.   0.07 0.   0.06 0.02 0.43 0.04 0.   0.02 0.   0.   0.   0.15 0.03 \\\r\n",
    " 0.02 0.   0.08 0.   0.01 0.07 0.   0.07 0.13 0.02 0.11 0.   0.21 0. \\\r\n",
    " 0.   0.   0.   0.   0.09 0.07 0.08 0.   0.08 0.01 0.03 0.02 0.03 0.03 \\\r\n",
    " 0.01 0.   0.17 0.   0.33 0.   0.01 0.04 0.   0.   0.   0.4  0.   0.02 \\\r\n",
    " 0.02 0.   0.07 0.   0.02 0.   0.   0.   0.01 0.08 0.05 0.   0.01 0. \\\r\n",
    " 0.19 0.   0.08 0.   0.31 0.   0.   0.   0.   0.   0.12 0.   0.01 0. \\\r\n",
    " 0.2  0.08 0.   0.47 0.97 0.73 0.99 0.73 0.95 0.71 0.94 0.63 0.91 0.31 \\\r\n",
    " 1.   0.55 1.   0.02 0.97 0.54 0.53 0.98 0.24 1.   0.06 0.96 0.62 0.98 \\\r\n",
    " 0.47 1.   0.41 1.   0.63 1.   0.78 0.92 0.8  1.   0.73 0.96 0.29 0.95 \\\r\n",
    " 0.74 0.97 0.69 0.98 0.71 1.   0.48 0.95 0.78 1.   0.66 1.   0.76 0.97 \\\r\n",
    " 0.32 1.   0.56 0.99 0.58 0.99 0.7  0.94 0.61 1.   0.64 0.96 0.39 1. \\\r\n",
    " 0.52 0.99 0.19 0.99 0.64 1.   0.55 0.98 0.66 0.97 0.28 0.98 0.68 1. \\\r\n",
    " 0.1  1.   0.64 0.92 0.73 0.99 0.36 1.   0.23 0.98 0.6  1.   0.13 1. \\\r\n",
    " 0.66 0.94 0.32 0.99 0.47 0.99 0.59 1.   0.04 1.   0.37 0.93 0.27 0.99 \\\r\n",
    " 0.75 0.09 1.   0.53 1.   0.76 1.   0.25 0.96 0.57 0.99 0.23 0.47 0.11 \\\r\n",
    " 0.28 0.57 0.49 0.29 0.6  0.52 0.56 0.45 0.59 0.66 0.29 0.53 0.22 0.2 \\\r\n",
    " 0.77 0.33 0.15 0.3  0.54 0.49 0.55 0.58 0.29 0.63 0.74 0.22 0.67 0.69 \\\r\n",
    " 0.54 0.55 0.62 0.26 0.08 0.33 0.38 0.4  0.58 0.47 0.73 0.78 0.16 0.64 \\\r\n",
    " 0.73 0.53 0.73 0.09 0.11 0.59 0.53 0.33 0.58 0.08 0.59 0.43 0.52 0.19 \\\r\n",
    " 0.2  0.45 0.35 0.22 0.   0.43 0.21 0.08 0.12 0.   0.01 0.3  0.76 0.08 \\\r\n",
    " 0.06 0.   0.09 0.39 0.38 0.05 0.26 0.01 0.   0.   0.04 0.01 0.   0.2 \\\r\n",
    " 0.   0.33 0.04 0.01 0.   0.   0.02 0.58 0.3  0.   0.17 0.48 0.08 0.09 \\\r\n",
    " 0.02 0.08 0.   0.13 0.02 0.04 0.05 0.25 0.   0.   0.   0.17 0.09 0.01 \\\r\n",
    " 0.04 0.01 0.05 0.04 0.67 0.   0.09 0.07 0.   0.   0.01 0.11 0.05 0.48 \\\r\n",
    " 0.   0.   0.   0.32 0.07 0.21 0.31 0.13 0.01 0.02 0.15 0.04 0.   0. \\\r\n",
    " 0.14 0.2  0.   0.03 0.01 0.01 0.25 0.03 0.19 0.04 0.   0.   0.02 0.5 \\\r\n",
    " 0.18 0.04 0.   0.06 0.37 0.   0.11 0.   0.17 0.08 0.   0.06 0.01 0.01 \\\r\n",
    " 0.57 0.   0.01 0.1  0.1  0.   0.16 0.   0.   0.25 0.12 0.11 0.19 0.14 \\\r\n",
    " 0.   0.12 0.   0.06 0.   0.07 0.55 0.14 0.12 0.14 0.4  0.11 0.4  0. \\\r\n",
    " 0.17 0.02 0.   0.03 0.06 0.09 0.1  0.   0.2  0.   0.01 0.03 0.03 0.66 \\\r\n",
    " 0.05 0.   0.07 0.25 0.   0.1  0.07 0.01 0.35 1.   0.95 1.   0.92 0.99 \\\r\n",
    " 0.9  0.91 0.98 0.91 0.9  0.95 0.9  0.9  0.9  0.94 0.93 1.   1.   0.99 \\\r\n",
    " 0.99 1.   0.99 1.   1.   1.   0.93 0.9  0.61 0.64 0.47 0.78 0.73 0.76 \\\r\n",
    " 0.76 0.54 0.36 0.15 0.43 0.61 0.31 0.72 0.51 0.29 0.18 0.14 0.18 0.38 \\\r\n",
    " 0.57 0.68 0.68 0.73 0.6  0.67 0.42 0.64 0.4  0.62 0.42 0.44 0.32 0.73 \\\r\n",
    " 0.19 0.41 0.36 0.32 0.39 0.62 0.77 0.65 0.34 0.48 0.62 0.29 0.72 0.59 \\\r\n",
    " 0.35 0.56 0.12 0.09 0.79 0.67 0.76 0.49 0.76 0.75 0.66 0.77 0.32 0.72 \\\r\n",
    " 0.63 0.62 0.51 0.67 0.44 0.72 0.04 0.79 0.05 0.81 0.01 0.05 0.37 0.57 \\\r\n",
    " 0.6  0.65 0.58 0.7  0.32 0.66 0.1  0.19 0.71 0.43 0.46 0.56 0.12 0.62 \\\r\n",
    " 0.73 0.26 0.2  0.36 0.78 0.74 0.99 0.97 0.98 0.98 1.   1.   0.91 1. \\\r\n",
    " 1.   0.99 1.   0.95 0.95 1.   1.   0.99 1.   1.   1.   1.   1.   0.98 \\\r\n",
    " 1.   0.99 1.   1.   1.   1.   0.99 0.99 0.97 1.   0.99 1.   1.   1. \\\r\n",
    " 1.   0.96 1.   0.94 1.   1.   1.   1.   0.94 1.   1.   0.95 1.   1. \\\r\n",
    " 1.   0.99 1.   0.94 0.99 1.   0.97 0.99 1.   0.98]'[1:-1].split()\r\n",
    "preds_train = [float(f) for f in preds_train]\r\n",
    "preds_test = '[0.44 0.98 0.17 0.23 0.68 0.34 0.45 0.49 0.33 0.51 0.25 0.41 0.22 0.58 \\\r\n",
    " 0.51 0.49 0.86 0.84 0.24 0.29 0.04 0.89 0.55 0.39 0.21 0.55 0.11 0.78 \\\r\n",
    " 0.82 0.33 0.53 0.17 0.55 0.81 0.16 0.33 0.92 0.82 0.01 0.66 0.06 0.11 \\\r\n",
    " 0.76 0.75 0.26 0.75 0.1  0.6  0.34 0.73 0.02 0.73 0.09 0.07 0.39 0.15 \\\r\n",
    " 0.14 0.93 0.06 0.37 0.02 0.55 0.1  0.76 0.1  0.43 0.39 0.45 0.34 0.25 \\\r\n",
    " 0.36 0.28 0.13 0.29 0.76 0.85 0.84 0.75 0.84 0.65 0.85 0.42 0.31 0.05 \\\r\n",
    " 0.38 0.4  0.5  0.07 0.38 0.21 0.36 0.72 0.53 0.66 0.23 0.81 0.91 0.13 \\\r\n",
    " 0.5  0.97 0.84 0.47 0.28 0.77 0.39 0.67 0.02 0.39 0.81 0.42 0.08 0.85 \\\r\n",
    " 0.48 0.61 0.28 0.22 0.84 0.25 0.81 0.46 0.87 0.12 0.77 0.16 0.18 0.68 \\\r\n",
    " 0.7  0.57 0.75 0.73 0.37 1.   0.92 0.86 0.12 0.82 0.66 0.76 0.25 0.48 \\\r\n",
    " 0.28 0.31 0.35 0.26 0.15 0.31 0.1  0.93 0.67 0.81 0.32 0.88 0.96 0.78 \\\r\n",
    " 0.34 0.53 0.08 0.75 0.71 0.15 0.98 0.15 0.36 0.74 0.79 0.56 0.35 0.82 \\\r\n",
    " 0.   0.09 0.82 0.98 0.63 0.36 0.51 0.9  0.85 0.6  0.54 0.22 0.81 0.27 \\\r\n",
    " 0.88 0.93 0.57 0.1  0.92 0.6  0.29 0.38 0.82 0.85 0.86 0.24 0.3  0.02 \\\r\n",
    " 0.92 0.21 0.32 0.38 0.24 0.53 0.81 0.89 0.54 0.85 0.77 0.54 0.45 0.57 \\\r\n",
    " 0.64 0.59 0.82 0.7  0.46 0.49 0.72 0.32 0.12 0.34 0.32 0.13 0.75 0.7 \\\r\n",
    " 0.23 0.1  0.23 0.76 0.6  0.4  0.12 0.32 0.85 0.19 0.36 0.2  0.8  0.08 \\\r\n",
    " 0.59 0.56 0.15 0.51 0.96 0.25 0.34 0.36 0.69 0.48 0.16 0.93 0.72 0.97 \\\r\n",
    " 0.34 0.61 0.98 0.78 0.88 0.23 0.78 0.08 0.17 0.19 0.8  0.9  0.57 0.89 \\\r\n",
    " 0.95 0.69 0.77 0.71 0.47 0.56 0.5  0.82 0.72 0.83 0.53 0.75 0.02 0.89 \\\r\n",
    " 0.86 0.26 0.43 0.24 0.99 0.99 0.4  0.84 0.91 0.4  0.57 0.56 0.96 0.62 \\\r\n",
    " 0.56 0.73 0.91 0.25 0.51 0.94 0.82 0.67 0.26 0.07 0.9  0.6  0.97 0.59 \\\r\n",
    " 0.47 0.15 0.56 0.69 0.77 0.01 0.06 0.07 0.45 0.06 0.94 0.64 0.99 0.99 \\\r\n",
    " 1.   0.97 0.86 0.91 0.73 0.23 0.04 0.   0.42 0.17 0.01 0.02 0.   0.44 \\\r\n",
    " 0.06 0.02 0.   0.63 0.19 0.02 0.22 0.44 0.19 0.   0.   0.1  0.01 0.88 \\\r\n",
    " 0.09 0.01 0.69 0.   0.03 0.12 0.44 0.29 0.11 0.03 0.   0.02 0.03 0.05 \\\r\n",
    " 0.41 0.27 0.28 0.73 0.   0.5  0.36 0.03 0.   0.   0.41 0.04 0.   0. \\\r\n",
    " 0.23 0.09 0.09 0.37 0.61 0.03 0.08 0.22 0.13 0.07 0.13 0.79 0.06 0.66 \\\r\n",
    " 0.89 0.02 0.39 0.09 0.26 0.1  0.92 0.85 0.02 0.84 0.37 0.16 0.94 0.6 \\\r\n",
    " 0.83 0.14 0.5  0.78 0.   0.58 0.01 0.88 0.85 0.56 0.04]'[1:-1].split()\r\n",
    "preds_test = [float(f) for f in preds_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = datasets[0][3]\r\n",
    "y_test = datasets[0][4]\r\n",
    "y_train_pred = np.array(preds_train) >= 0.5\r\n",
    "y_test_pred = np.array(preds_test) >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: accuracy = 0.7971246006389776, balanced = 0.8677083333333333, f1 = 0.6968973747016707\n",
      "test: accuracy = 0.5587529976019184, balanced = 0.5580446520657647, f1 = 0.39072847682119205\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\r\n",
    "print(f'train: accuracy = {accuracy_score(y_train, y_train_pred)}, balanced = {balanced_accuracy_score(y_train, y_train_pred)}, f1 = {f1_score(y_train, y_train_pred)}')\r\n",
    "print(f'test: accuracy = {accuracy_score(y_test, y_test_pred)}, balanced = {balanced_accuracy_score(y_test, y_test_pred)}, f1 = {f1_score(y_test, y_test_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(scores)\r\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.filter(regex='.*Testing Balanced.*', axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.filter(regex='.*Testing Balanced.*', axis='index').T.plot(kind='bar')\r\n",
    "plt.title('Not really clear which classifier is the best')\r\n",
    "plt.legend(bbox_to_anchor=(1.05,1))\r\n",
    "plt.show()\r\n",
    "\r\n",
    "scores_df.filter(regex='.*Testing Balanced.*', axis='index').plot(kind='bar')\r\n",
    "plt.title('Using matched pairs seems a little bit better than using unbalanced data')\r\n",
    "plt.legend(bbox_to_anchor=(1.05,1))\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('classifier_scores', scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('translational': conda)",
   "name": "python394jvsc74a57bd01edce0b25732ef697872fcbeacb22edaf72c3789e6a34e8653bd03b41c1dfc41"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}